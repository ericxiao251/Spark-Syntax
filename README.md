# Spark-Syntax

This is a public repo documenting all of the "best practices" of writing PySpark code from what I have learnt from working with `PySpark` for 3 years. This will mainly focus on the `Spark DataFrames and SQL` library.

# Table of Contexts:

## Chapter 1 - Getting Started with Spark:
* #### 1.1 - [Useful Material](https://github.com/ericxiao251/spark-syntax/blob/master/src/Chapter%201%20-%20Basics/Section%201%20-%20Useful%20Material.md)
* #### 1.2 - [Creating your First DataFrame](https://github.com/ericxiao251/spark-syntax/blob/master/src/Chapter%201%20-%20Basics/Section%202%20-%20Creating%20your%20First%20Data%20Object.ipynb)
* #### 1.3 - [Reading your First Dataset](https://github.com/ericxiao251/spark-syntax/blob/master/src/Chapter%201%20-%20Basics/Section%203%20-%20Reading%20your%20First%20Dataset.ipynb)
* #### 1.4 - [More Comfortable with SQL?](https://github.com/ericxiao251/spark-syntax/blob/master/src/Chapter%201%20-%20Basics/Section%204%20-%20More%20Comfortable%20with%20SQL%3F.ipynb)

## Chapter 2 - Exploring the Spark APIs:
* #### 2.1 - [Performing your First Transformations](https://github.com/ericxiao251/spark-syntax/blob/master/src/Chapter%201%20-%20Basics/Section%203%20-%20Performing%20your%20First%20Transformations.ipynb)
    * ##### 2.1.1 - Looking at your data
    * ##### 2.1.2 - Selecting a Subset of Columns
    * ##### 2.1.3 - Filtering the data
    * ##### 2.1.4 - Case Statements
    * ##### 2.1.5 - Filling in Null Values (fillna/colasce)
    * ##### 2.1.6 - How to Properly Union Your DataFrames
    * ##### 2.1.7 - [Equalities with Null Values](https://github.com/ericxiao251/spark-syntax/blob/master/src/Chapter%202%20-%20Exploring%20the%20Spark%20APIs/Section%206%20-%20Equalities%20with%20Null%20Values.ipynb)
* #### 2.2 - [Spark Functions aren't Enough, I Need my Own Functions! (WIP)]()
* #### 2.3 - [What the Heck are Spark Literals?](https://github.com/ericxiao251/spark-syntax/blob/master/src/Chapter%202%20-%20Exploring%20the%20Spark%20APIs/Section%203%20-%20What%20the%20Heck%20are%20Spark%20Literals%3F.ipynb)
* #### 2.4 - [Working with Decimal Types (WIP)]()
* #### 2.5 - [Why did my Decimals overflow :( (WIP)]()

## Chapter 3 - Joins:
* #### 3.1 - [A Basic Join](https://github.com/ericxiao251/spark-syntax/blob/master/src/Chapter%203%20-%20Joins/Section%201%20-%20A%20Basic%20Join.ipynb)
* #### 3.2 - [Improving Joins](https://github.com/ericxiao251/spark-syntax/blob/master/src/Chapter%203%20-%20Joins/Section%202%20-%20Imporving%20Joins.ipynb)
* #### 3.3 - [Joining on Skewed Data](https://github.com/ericxiao251/spark-syntax/blob/master/src/Chapter%203%20-%20Joins/Section%203%20-%20Joins%20on%20Skewed%20Data.ipynb)
* #### 3.4 - [Range Joins Conditions (WIP)](https://github.com/ericxiao251/spark-syntax/blob/master/src/Chapter%203%20-%20Joins/Section%204%20-%20Range%20Join%20Conditions%20%5BTODO%5D.ipynb)

## Chapter 4 - Aggregates:
* #### 4.1 - [Clean Aggregations](https://github.com/ericxiao251/spark-syntax/blob/master/src/Chapter%204%20-%20Aggregates/Section%201%20-%20Clean%20Aggregations.ipynb)
* #### 4.2 - [Non Deterministic Behaviours](https://github.com/ericxiao251/spark-syntax/blob/master/src/Chapter%204%20-%20Aggregates/Section%202%20-%20Non%20Deterministic%20Behaviours.ipynb)

## Chapter 5 - Window Objects:
* #### 5.1 - [Default Behaviors of Window Objects](https://github.com/ericxiao251/spark-syntax/blob/master/src/Chapter%205%20-%20Window%20Objects/Section%201%20-%20Default%20Behaviours%20of%20OrderBy%20on%20a%20Window%20Object.ipynb)
* #### 5.2 - [Proper Ordering of Rows Using a Window Object](https://github.com/ericxiao251/spark-syntax/blob/master/src/Chapter%205%20-%20Aggregates/Section%202%20-%20High%20Frequency%20Data.ipynb)

## Chapter 6 - Error Logs:

## Chapter 6 - Tuning & Spark Parameters:
